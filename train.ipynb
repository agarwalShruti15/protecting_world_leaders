{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Pipeline\n",
    "\n",
    "In this notebook we demonstrate how to train using the video database. \n",
    "\n",
    "Requirements: \n",
    "\n",
    "<ul>\n",
    "    <li> Path to OpenFace2.0 build/bin folder. This folder is created after OpenFace compilation. Please follow the instructions given <a href=\"https://github.com/TadasBaltrusaitis/OpenFace/wiki\">here</a> to compile OpenFace2.0 on your machine.\n",
    "     <li> Path to the POI specific videos to train the one-class novelity detection model. The video should be cleaned as per the specifications given in README.md\n",
    "     <li> Path to save the output trained model.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "import os\n",
    "import utils as u\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Initialize\n",
    "\n",
    "\n",
    "#### Hyper-parameters\n",
    "There are few hyper-parameters that we need to set before training. The value of these parameters are currently set to the one used in the paper.pdf. \n",
    "\n",
    "<ul>\n",
    "    <li> clip_length: length in number of frames over which the correlation is computed. Currently set to 300 which corresponds to 10-second length for a video saved at 30 fps.\n",
    "    <li> shift_win: the number of frames to shift to extract the next overlapping video clip. \n",
    "    <li> c_t: the confidence threshold below which the feature estimated by OpenFace are rejected\n",
    "</ul>\n",
    "\n",
    "#### Paths\n",
    "\n",
    "<ul>\n",
    "    <li> open_face_path: path to OpenFace build/bin folder.\n",
    "    <li> videos_path: the path to all the .mp4 videos for a POI on which we train the SVM model.\n",
    "    <li> model_out_path: the output path to store the model file.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "clip_len = 300 #length of the clip in frames, assume the fps=30\n",
    "shift_win = 5 #the number of frames to shift\n",
    "c_t = 0.93 #the confidence threshold for OpenFace detection. Even a single less confident frame will reject the entire 10-second clip\n",
    "\n",
    "#paths\n",
    "open_face_path = 'OpenFace' #path to OpenFace build/bin folder\n",
    "videos_path = '/Users/shruti_agarwal/Dropbox/data/projects/deep_fakes/videos/bo' #the path to all the .mp4 videos for a POI on which we train the SVM model\n",
    "model_out_path = 'models/bo' # the output path to store the model file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Select the features\n",
    "\n",
    "By default we train the model using the entire 190-D correlation features. The names of all the pairs of features can be extracted using u.get_all() function. \n",
    "\n",
    "We can also train using only selective features. For example, in case of Obama we found top 29 features to train the model. The names of these features are stored in the .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AU01_rAU02_r' 'AU01_rAU04_r' 'AU01_rAU05_r' 'AU01_rAU06_r'\n",
      " 'AU01_rAU07_r' 'AU01_rAU09_r' 'AU01_rAU10_r' 'AU01_rAU12_r'\n",
      " 'AU01_rAU14_r' 'AU01_rAU15_r' 'AU01_rAU17_r' 'AU01_rAU20_r'\n",
      " 'AU01_rAU23_r' 'AU01_rAU25_r' 'AU01_rAU26_r' 'AU01_rpose_Rx'\n",
      " 'AU01_rpose_Rz' 'AU01_rlip_ver' 'AU01_rlip_hor' 'AU02_rAU04_r'\n",
      " 'AU02_rAU05_r' 'AU02_rAU06_r' 'AU02_rAU07_r' 'AU02_rAU09_r'\n",
      " 'AU02_rAU10_r' 'AU02_rAU12_r' 'AU02_rAU14_r' 'AU02_rAU15_r'\n",
      " 'AU02_rAU17_r' 'AU02_rAU20_r' 'AU02_rAU23_r' 'AU02_rAU25_r'\n",
      " 'AU02_rAU26_r' 'AU02_rpose_Rx' 'AU02_rpose_Rz' 'AU02_rlip_ver'\n",
      " 'AU02_rlip_hor' 'AU04_rAU05_r' 'AU04_rAU06_r' 'AU04_rAU07_r'\n",
      " 'AU04_rAU09_r' 'AU04_rAU10_r' 'AU04_rAU12_r' 'AU04_rAU14_r'\n",
      " 'AU04_rAU15_r' 'AU04_rAU17_r' 'AU04_rAU20_r' 'AU04_rAU23_r'\n",
      " 'AU04_rAU25_r' 'AU04_rAU26_r' 'AU04_rpose_Rx' 'AU04_rpose_Rz'\n",
      " 'AU04_rlip_ver' 'AU04_rlip_hor' 'AU05_rAU06_r' 'AU05_rAU07_r'\n",
      " 'AU05_rAU09_r' 'AU05_rAU10_r' 'AU05_rAU12_r' 'AU05_rAU14_r'\n",
      " 'AU05_rAU15_r' 'AU05_rAU17_r' 'AU05_rAU20_r' 'AU05_rAU23_r'\n",
      " 'AU05_rAU25_r' 'AU05_rAU26_r' 'AU05_rpose_Rx' 'AU05_rpose_Rz'\n",
      " 'AU05_rlip_ver' 'AU05_rlip_hor' 'AU06_rAU07_r' 'AU06_rAU09_r'\n",
      " 'AU06_rAU10_r' 'AU06_rAU12_r' 'AU06_rAU14_r' 'AU06_rAU15_r'\n",
      " 'AU06_rAU17_r' 'AU06_rAU20_r' 'AU06_rAU23_r' 'AU06_rAU25_r'\n",
      " 'AU06_rAU26_r' 'AU06_rpose_Rx' 'AU06_rpose_Rz' 'AU06_rlip_ver'\n",
      " 'AU06_rlip_hor' 'AU07_rAU09_r' 'AU07_rAU10_r' 'AU07_rAU12_r'\n",
      " 'AU07_rAU14_r' 'AU07_rAU15_r' 'AU07_rAU17_r' 'AU07_rAU20_r'\n",
      " 'AU07_rAU23_r' 'AU07_rAU25_r' 'AU07_rAU26_r' 'AU07_rpose_Rx'\n",
      " 'AU07_rpose_Rz' 'AU07_rlip_ver' 'AU07_rlip_hor' 'AU09_rAU10_r'\n",
      " 'AU09_rAU12_r' 'AU09_rAU14_r' 'AU09_rAU15_r' 'AU09_rAU17_r'\n",
      " 'AU09_rAU20_r' 'AU09_rAU23_r' 'AU09_rAU25_r' 'AU09_rAU26_r'\n",
      " 'AU09_rpose_Rx' 'AU09_rpose_Rz' 'AU09_rlip_ver' 'AU09_rlip_hor'\n",
      " 'AU10_rAU12_r' 'AU10_rAU14_r' 'AU10_rAU15_r' 'AU10_rAU17_r'\n",
      " 'AU10_rAU20_r' 'AU10_rAU23_r' 'AU10_rAU25_r' 'AU10_rAU26_r'\n",
      " 'AU10_rpose_Rx' 'AU10_rpose_Rz' 'AU10_rlip_ver' 'AU10_rlip_hor'\n",
      " 'AU12_rAU14_r' 'AU12_rAU15_r' 'AU12_rAU17_r' 'AU12_rAU20_r'\n",
      " 'AU12_rAU23_r' 'AU12_rAU25_r' 'AU12_rAU26_r' 'AU12_rpose_Rx'\n",
      " 'AU12_rpose_Rz' 'AU12_rlip_ver' 'AU12_rlip_hor' 'AU14_rAU15_r'\n",
      " 'AU14_rAU17_r' 'AU14_rAU20_r' 'AU14_rAU23_r' 'AU14_rAU25_r'\n",
      " 'AU14_rAU26_r' 'AU14_rpose_Rx' 'AU14_rpose_Rz' 'AU14_rlip_ver'\n",
      " 'AU14_rlip_hor' 'AU15_rAU17_r' 'AU15_rAU20_r' 'AU15_rAU23_r'\n",
      " 'AU15_rAU25_r' 'AU15_rAU26_r' 'AU15_rpose_Rx' 'AU15_rpose_Rz'\n",
      " 'AU15_rlip_ver' 'AU15_rlip_hor' 'AU17_rAU20_r' 'AU17_rAU23_r'\n",
      " 'AU17_rAU25_r' 'AU17_rAU26_r' 'AU17_rpose_Rx' 'AU17_rpose_Rz'\n",
      " 'AU17_rlip_ver' 'AU17_rlip_hor' 'AU20_rAU23_r' 'AU20_rAU25_r'\n",
      " 'AU20_rAU26_r' 'AU20_rpose_Rx' 'AU20_rpose_Rz' 'AU20_rlip_ver'\n",
      " 'AU20_rlip_hor' 'AU23_rAU25_r' 'AU23_rAU26_r' 'AU23_rpose_Rx'\n",
      " 'AU23_rpose_Rz' 'AU23_rlip_ver' 'AU23_rlip_hor' 'AU25_rAU26_r'\n",
      " 'AU25_rpose_Rx' 'AU25_rpose_Rz' 'AU25_rlip_ver' 'AU25_rlip_hor'\n",
      " 'AU26_rpose_Rx' 'AU26_rpose_Rz' 'AU26_rlip_ver' 'AU26_rlip_hor'\n",
      " 'pose_Rxpose_Rz' 'pose_Rxlip_ver' 'pose_Rxlip_hor' 'pose_Rzlip_ver'\n",
      " 'pose_Rzlip_hor' 'lip_verlip_hor']\n"
     ]
    }
   ],
   "source": [
    "n = 190 #number of features used, for Obama use n=29 features\n",
    "feat_file = 'models/ablation_obama_bo_smalldiffbo_UWfakebo_faceswapbo_imposter.csv'\n",
    "\n",
    "if n == 190:\n",
    "    feat_nm = u.get_all()\n",
    "else:\n",
    "    feat_nm = u.get_feat_from_file(feat_file, n)\n",
    "    \n",
    "# get the correlation feature names\n",
    "feat_nm = np.array([''.join(x) for x in feat_nm])\n",
    "\n",
    "print(feat_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) extract the features\n",
    "\n",
    "For all the videos in the train folder extract the above selected correlation features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the videos\n",
    "vid_nms = [os.path.join(videos_path, f) for f in os.listdir(videos_path) if f.endswith('.mp4')]\n",
    "\n",
    "#get all the features in a dictionary\n",
    "features = {}\n",
    "for v in vid_nms:\n",
    "\n",
    "    #OpenFace\n",
    "    face_features = u.get_facial_features(v, open_face_path, c_t)\n",
    "    \n",
    "    #Correlation\n",
    "    features[v] = u.get_corr_per_frame(face_features, clip_len, shift_win)[feat_nm]\n",
    "    \n",
    "X_train = pd.concat(list(features.values()), ignore_index=True, sort=False)\n",
    "\n",
    "print('train length {}'.format(len(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) train the model\n",
    "\n",
    "Train the one class SVM model using the above computed features. The model is trained using rbf kernel with default values for gamma and nu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training...\n",
      "done training in 369.75785088539124 sec\n",
      "Score range min: 8.961812240713925 max: 28.077287679223335\n"
     ]
    }
   ],
   "source": [
    "#train SVM\n",
    "print('begin training...')\n",
    "start = time.time()\n",
    "model = u.train_ovr(np.array(X_train))\n",
    "end_time = time.time()\n",
    "print('done training in {} sec'.format(end_time-start))\n",
    "\n",
    "#score range for traininf data\n",
    "pred_prob = model['model'].score_samples(model['scaler'].transform(np.array(X_train)))\n",
    "prob_rng = [np.quantile(pred_prob, q=0.02), np.quantile(pred_prob, q=0.98)] # this range is used to normalize the test scores\n",
    "print('Score range min: {} max: {}'.format(prob_rng[0], prob_rng[1]))\n",
    "\n",
    "#save the model with all the parameters to be used for testing\n",
    "model['prob_rng'] = prob_rng\n",
    "model['clip_len'] = clip_len\n",
    "model['feat_name'] = feat_nm\n",
    "model['shift_win'] = shift_win\n",
    "model['c_t'] = c_t\n",
    "\n",
    "#save the model\n",
    "u.save_obj(model, os.path.dirname(model_out_path), os.path.basename(model_out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
